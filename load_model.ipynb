{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load saved model\n",
    "\n",
    "from model import ner_bio\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from pytorch_pretrained_bert import BertModel,BertConfig, BertForPreTraining\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "import csv\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForTokenClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch.nn as nn\n",
    "MAX_LEN = 75\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = BertTokenizer(vocab_file='biobert_v1.0_pubmed_pmc/vocab.txt', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_retriver(path):\n",
    "    \n",
    "    with open(path) as tsvfile:\n",
    "        reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "        sentences = []\n",
    "        tags = []\n",
    "        sent = []\n",
    "        tag = []\n",
    "        for row in reader:\n",
    "            if len(row) == 0:\n",
    "                if len(sent) != len(tag):\n",
    "                    print('Error')\n",
    "                    break\n",
    "                sentences.append(sent)\n",
    "                tags.append(tag)\n",
    "                sent = []\n",
    "                tag = []\n",
    "            else:\n",
    "                sent.append(row[0])\n",
    "                tag.append(row[1])\n",
    "            \n",
    "    return sentences, tags\n",
    "\n",
    "def tokenize_and_preserve_labels(sentence, text_labels):\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    for word, label in zip(sentence, text_labels):\n",
    "\n",
    "        # Tokenizing the words\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        \n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('tags_small.csv')\n",
    "tag_values = data['tags'].values\n",
    "vocab_len = len(tag_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I-Cellular_component</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E-Gene_or_gene_product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I-Organism_subdivision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I-Organism_substance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B-Gene_or_gene_product</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     tags\n",
       "0    I-Cellular_component\n",
       "1  E-Gene_or_gene_product\n",
       "2  I-Organism_subdivision\n",
       "3    I-Organism_substance\n",
       "4  B-Gene_or_gene_product"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()\n",
    "# len(tag_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "rootdir = './BioNLP'\n",
    "import os\n",
    "import csv\n",
    "sentences = []\n",
    "tags = []\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        if file == 'test.tsv':\n",
    "#             print(os.path.join(subdir, file))\n",
    "            path_ = os.path.join(subdir, file)\n",
    "            sent, tag =sentence_retriver(path_)\n",
    "            sentences.extend(sent)\n",
    "            tags.extend(tag)\n",
    "            \n",
    "sentences = sentences[0:5000]\n",
    "tags = tags[0:5000]\n",
    "\n",
    "tag_list = []\n",
    "for idx,tg in enumerate(tags):\n",
    "    for t in tg:\n",
    "        if t != 'O':\n",
    "            tag_list.append(idx)\n",
    "        break\n",
    "        \n",
    "test_sentences = []\n",
    "test_labels =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hfq -- B-Protein \n",
      "\n",
      "was -- O \n",
      "\n",
      "observed -- O \n",
      "\n",
      "to -- O \n",
      "\n",
      "modulate -- O \n",
      "\n",
      "numerous -- O \n",
      "\n",
      "proteins -- O \n",
      "\n",
      "involved -- O \n",
      "\n",
      "in -- O \n",
      "\n",
      "envelope -- O \n",
      "\n",
      "stress -- O \n",
      "\n",
      "under -- O \n",
      "\n",
      "AMM -- O \n",
      "\n",
      "- -- O \n",
      "\n",
      "2 -- O \n",
      "\n",
      "conditions -- O \n",
      "\n",
      ", -- O \n",
      "\n",
      "including -- O \n",
      "\n",
      "FkpA -- B-Protein \n",
      "\n",
      ", -- O \n",
      "\n",
      "SurA -- B-Protein \n",
      "\n",
      ", -- O \n",
      "\n",
      "HtrA -- B-Protein \n",
      "\n",
      ", -- O \n",
      "\n",
      "NlpB -- B-Protein \n",
      "\n",
      ", -- O \n",
      "\n",
      "NmpC -- B-Protein \n",
      "\n",
      ", -- O \n",
      "\n",
      "ClpA -- B-Protein \n",
      "\n",
      ", -- O \n",
      "\n",
      "SlyD -- B-Protein \n",
      "\n",
      ", -- O \n",
      "\n",
      "RseA -- B-Protein \n",
      "\n",
      ", -- O \n",
      "\n",
      "RseB -- B-Protein \n",
      "\n",
      ", -- O \n",
      "\n",
      "RpoE -- B-Protein \n",
      "\n",
      ", -- O \n",
      "\n",
      "OmpF -- B-Protein \n",
      "\n",
      ", -- O \n",
      "\n",
      "and -- O \n",
      "\n",
      "HtpG -- B-Protein \n",
      "\n",
      ", -- O \n",
      "\n",
      "which -- O \n",
      "\n",
      "were -- O \n",
      "\n",
      "all -- O \n",
      "\n",
      "up -- O \n",
      "\n",
      "- -- O \n",
      "\n",
      "regulated -- O \n",
      "\n",
      "in -- O \n",
      "\n",
      "the -- O \n",
      "\n",
      "Deltahfq -- B-Protein \n",
      "\n",
      "mutant -- O \n",
      "\n",
      "strain -- O \n",
      "\n",
      ". -- O \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "test_idx = random.choice(tag_list)\n",
    "\n",
    "# test_idx = 9999\n",
    "test_sentence = sentences[test_idx]\n",
    "test_label = tags[test_idx]\n",
    "\n",
    "test_sentences.append(test_sentence)\n",
    "test_labels.append(test_labels)\n",
    "for l,s in zip(test_sentence,test_label):\n",
    "    print(l,'--',s,'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hfq was observed to modulate numerous proteins involved in envelope stress under AMM - 2 conditions , including FkpA , SurA , HtrA , NlpB , NmpC , ClpA , SlyD , RseA , RseB , RpoE , OmpF , and HtpG , which were all up - regulated in the Deltahfq mutant strain . \n",
      "\n",
      "[[...], [...], [...], [...]]\n"
     ]
    }
   ],
   "source": [
    "# print(len(test_sentences),len(test_labels))\n",
    "i=-1\n",
    "print(\" \".join(test_sentences[i]),'\\n')\n",
    "print(test_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Regulation of Saccharomyces cerevisiae kinetochores by the type 1 phosphatase Glc7p.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "sent_text = nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Regulation of Saccharomyces cerevisiae kinetochores by the type 1 phosphatase Glc7p.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sent_text:\n",
    "    tokenized_text = nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Regulation',\n",
       " 'of',\n",
       " 'Saccharomyces',\n",
       " 'cerevisiae',\n",
       " 'kinetochores',\n",
       " 'by',\n",
       " 'the',\n",
       " 'type',\n",
       " '1',\n",
       " 'phosphatase',\n",
       " 'Glc7p',\n",
       " '.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Directed cell migration is critical across biological processes spanning healing to cancer invasion, yet no existing tools allow real-time interactive guidance over such migration. We present a new bioreactor that harnesses electrotaxis—directed cell migration along electric field gradients—by integrating four independent electrodes under computer control to dynamically program electric field patterns, and hence steer cell migration. Using this platform, we programmed and characterized multiple precise, two-dimensional collective migration maneuvers in renal epithelia and primary skin keratinocyte ensembles. First, we demonstrated on-demand, 90-degree collective turning. Next, we developed a universal electrical stimulation scheme capable of programming arbitrary 2D migration maneuvers such as precise angular turns and migration in a complete circle. Our stimulation scheme proves that cells effectively time-average electric field cues, helping to elucidate the transduction timescales in electrotaxis. Together, this work represents an enabling platform for controlling cell migration with broad utility across many cell types.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_text = nltk.sent_tokenize(text)\n",
    "tokenized_text = []\n",
    "for sentence in sent_text:\n",
    "    tokenized_text.append(nltk.word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Directed', 'cell', 'migration', 'is', 'critical', 'across', 'biological', 'processes', 'spanning', 'healing', 'to', 'cancer', 'invasion', ',', 'yet', 'no', 'existing', 'tools', 'allow', 'real-time', 'interactive', 'guidance', 'over', 'such', 'migration', '.'], ['We', 'present', 'a', 'new', 'bioreactor', 'that', 'harnesses', 'electrotaxis—directed', 'cell', 'migration', 'along', 'electric', 'field', 'gradients—by', 'integrating', 'four', 'independent', 'electrodes', 'under', 'computer', 'control', 'to', 'dynamically', 'program', 'electric', 'field', 'patterns', ',', 'and', 'hence', 'steer', 'cell', 'migration', '.'], ['Using', 'this', 'platform', ',', 'we', 'programmed', 'and', 'characterized', 'multiple', 'precise', ',', 'two-dimensional', 'collective', 'migration', 'maneuvers', 'in', 'renal', 'epithelia', 'and', 'primary', 'skin', 'keratinocyte', 'ensembles', '.'], ['First', ',', 'we', 'demonstrated', 'on-demand', ',', '90-degree', 'collective', 'turning', '.'], ['Next', ',', 'we', 'developed', 'a', 'universal', 'electrical', 'stimulation', 'scheme', 'capable', 'of', 'programming', 'arbitrary', '2D', 'migration', 'maneuvers', 'such', 'as', 'precise', 'angular', 'turns', 'and', 'migration', 'in', 'a', 'complete', 'circle', '.'], ['Our', 'stimulation', 'scheme', 'proves', 'that', 'cells', 'effectively', 'time-average', 'electric', 'field', 'cues', ',', 'helping', 'to', 'elucidate', 'the', 'transduction', 'timescales', 'in', 'electrotaxis', '.'], ['Together', ',', 'this', 'work', 'represents', 'an', 'enabling', 'platform', 'for', 'controlling', 'cell', 'migration', 'with', 'broad', 'utility', 'across', 'many', 'cell', 'types', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserve(sentence):\n",
    "    tokenized_sentence = []\n",
    "    \n",
    "    for word in sentence:\n",
    "        # Tokenizing the words\n",
    "        tokenized_word = tokenizer.tokenize(word)   \n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "    return tokenized_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts_ = [\n",
    "    tokenize_and_preserve(sent)\n",
    "    for sent in tokenized_text\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = [tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts_]\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[16698,\n",
       "  2765,\n",
       "  10348,\n",
       "  1110,\n",
       "  3607,\n",
       "  1506,\n",
       "  7269,\n",
       "  5669,\n",
       "  14827,\n",
       "  9523,\n",
       "  1106,\n",
       "  4182,\n",
       "  4923,\n",
       "  117,\n",
       "  1870,\n",
       "  1185,\n",
       "  3685,\n",
       "  5537,\n",
       "  2621,\n",
       "  1842,\n",
       "  118,\n",
       "  1159,\n",
       "  12196,\n",
       "  8815,\n",
       "  1166,\n",
       "  1216,\n",
       "  10348,\n",
       "  119],\n",
       " [1284,\n",
       "  1675,\n",
       "  170,\n",
       "  1207,\n",
       "  25128,\n",
       "  11811,\n",
       "  9363,\n",
       "  1115,\n",
       "  20211,\n",
       "  1279,\n",
       "  24266,\n",
       "  1777,\n",
       "  8745,\n",
       "  1116,\n",
       "  783,\n",
       "  2002,\n",
       "  2765,\n",
       "  10348,\n",
       "  1373,\n",
       "  3651,\n",
       "  1768,\n",
       "  19848,\n",
       "  1116,\n",
       "  783,\n",
       "  1118,\n",
       "  26975,\n",
       "  1300,\n",
       "  2457,\n",
       "  24266,\n",
       "  4704,\n",
       "  1223,\n",
       "  2775,\n",
       "  1654,\n",
       "  1106,\n",
       "  9652,\n",
       "  2716,\n",
       "  1788,\n",
       "  3651,\n",
       "  1768,\n",
       "  6692,\n",
       "  117,\n",
       "  1105,\n",
       "  7544,\n",
       "  25284,\n",
       "  2765,\n",
       "  10348,\n",
       "  119],\n",
       " [7993,\n",
       "  1142,\n",
       "  3482,\n",
       "  117,\n",
       "  1195,\n",
       "  18693,\n",
       "  1105,\n",
       "  6858,\n",
       "  2967,\n",
       "  10515,\n",
       "  117,\n",
       "  1160,\n",
       "  118,\n",
       "  8611,\n",
       "  7764,\n",
       "  10348,\n",
       "  27559,\n",
       "  1107,\n",
       "  1231,\n",
       "  7050,\n",
       "  174,\n",
       "  18965,\n",
       "  18809,\n",
       "  1465,\n",
       "  1105,\n",
       "  2425,\n",
       "  2241,\n",
       "  180,\n",
       "  5970,\n",
       "  20064,\n",
       "  3457,\n",
       "  1566,\n",
       "  24957,\n",
       "  119],\n",
       " [1752,\n",
       "  117,\n",
       "  1195,\n",
       "  7160,\n",
       "  1113,\n",
       "  118,\n",
       "  4555,\n",
       "  117,\n",
       "  3078,\n",
       "  118,\n",
       "  2178,\n",
       "  7764,\n",
       "  3219,\n",
       "  119],\n",
       " [5893,\n",
       "  117,\n",
       "  1195,\n",
       "  1872,\n",
       "  170,\n",
       "  8462,\n",
       "  6538,\n",
       "  23842,\n",
       "  5471,\n",
       "  4451,\n",
       "  1104,\n",
       "  4159,\n",
       "  16439,\n",
       "  22947,\n",
       "  10348,\n",
       "  27559,\n",
       "  1216,\n",
       "  1112,\n",
       "  10515,\n",
       "  17553,\n",
       "  3587,\n",
       "  1105,\n",
       "  10348,\n",
       "  1107,\n",
       "  170,\n",
       "  2335,\n",
       "  4726,\n",
       "  119],\n",
       " [3458,\n",
       "  23842,\n",
       "  5471,\n",
       "  17617,\n",
       "  1115,\n",
       "  3652,\n",
       "  5877,\n",
       "  1159,\n",
       "  118,\n",
       "  1903,\n",
       "  3651,\n",
       "  1768,\n",
       "  26602,\n",
       "  117,\n",
       "  4395,\n",
       "  1106,\n",
       "  8468,\n",
       "  21977,\n",
       "  6859,\n",
       "  1566,\n",
       "  1103,\n",
       "  14715,\n",
       "  11243,\n",
       "  1551,\n",
       "  20532,\n",
       "  1116,\n",
       "  1107,\n",
       "  24266,\n",
       "  1777,\n",
       "  8745,\n",
       "  1116,\n",
       "  119],\n",
       " [6333,\n",
       "  117,\n",
       "  1142,\n",
       "  1250,\n",
       "  5149,\n",
       "  1126,\n",
       "  12619,\n",
       "  3482,\n",
       "  1111,\n",
       "  9783,\n",
       "  2765,\n",
       "  10348,\n",
       "  1114,\n",
       "  4728,\n",
       "  10345,\n",
       "  1506,\n",
       "  1242,\n",
       "  2765,\n",
       "  3322,\n",
       "  119]]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[1])\n",
    "new_tokens, new_labels = [], []\n",
    "for token in tokens:\n",
    "    if token.startswith(\"##\"):\n",
    "        new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "    else:\n",
    "        \n",
    "        new_tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " 'present',\n",
       " 'a',\n",
       " 'new',\n",
       " 'bioreactor',\n",
       " 'that',\n",
       " 'harnesses',\n",
       " 'electrotaxis',\n",
       " '—',\n",
       " 'directed',\n",
       " 'cell',\n",
       " 'migration',\n",
       " 'along',\n",
       " 'electric',\n",
       " 'field',\n",
       " 'gradients',\n",
       " '—',\n",
       " 'by',\n",
       " 'integrating',\n",
       " 'four',\n",
       " 'independent',\n",
       " 'electrodes',\n",
       " 'under',\n",
       " 'computer',\n",
       " 'control',\n",
       " 'to',\n",
       " 'dynamically',\n",
       " 'program',\n",
       " 'electric',\n",
       " 'field',\n",
       " 'patterns',\n",
       " ',',\n",
       " 'and',\n",
       " 'hence',\n",
       " 'steer',\n",
       " 'cell',\n",
       " 'migration',\n",
       " '.']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[141, 178, 187, 174, 172, 189, 174, 173], [172, 174, 181, 181], [182, 178, 176, 187, 170, 189, 178, 184, 183], [178, 188], [172, 187, 178, 189, 178, 172, 170, 181], [170, 172, 187, 184, 188, 188], [171, 178, 184, 181, 184, 176, 178, 172, 170, 181], [185, 187, 184, 172, 174, 188, 188, 174, 188], [188, 185, 170, 183, 183, 178, 183, 176], [177, 174, 170, 181, 178, 183, 176], [189, 184], [172, 170, 183, 172, 174, 187], [178, 183, 191, 170, 188, 178, 184, 183], [117], [194, 174, 189], [183, 184], [174, 193, 178, 188, 189, 178, 183, 176], [189, 184, 184, 181, 188], [170, 181, 181, 184, 192], [187, 174, 170, 181, 118, 189, 178, 182, 174], [178, 183, 189, 174, 187, 170, 172, 189, 178, 191, 174], [176, 190, 178, 173, 170, 183, 172, 174], [184, 191, 174, 187], [188, 190, 172, 177], [182, 178, 176, 187, 170, 189, 178, 184, 183], [119]]\n"
     ]
    }
   ],
   "source": [
    "print(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Directed', 'cell', 'migration', 'is', 'critical', 'across', 'biological', 'processes', 'spanning', 'healing', 'to', 'cancer', 'invasion', ',', 'yet', 'no', 'existing', 'tools', 'allow', 'real-time', 'interactive', 'guidance', 'over', 'such', 'migration', '.']\n",
      "['We', 'present', 'a', 'new', 'bioreactor', 'that', 'harnesses', 'electrotaxis—directed', 'cell', 'migration', 'along', 'electric', 'field', 'gradients—by', 'integrating', 'four', 'independent', 'electrodes', 'under', 'computer', 'control', 'to', 'dynamically', 'program', 'electric', 'field', 'patterns', ',', 'and', 'hence', 'steer', 'cell', 'migration', '.']\n",
      "['Using', 'this', 'platform', ',', 'we', 'programmed', 'and', 'characterized', 'multiple', 'precise', ',', 'two-dimensional', 'collective', 'migration', 'maneuvers', 'in', 'renal', 'epithelia', 'and', 'primary', 'skin', 'keratinocyte', 'ensembles', '.']\n",
      "['First', ',', 'we', 'demonstrated', 'on-demand', ',', '90-degree', 'collective', 'turning', '.']\n",
      "['Next', ',', 'we', 'developed', 'a', 'universal', 'electrical', 'stimulation', 'scheme', 'capable', 'of', 'programming', 'arbitrary', '2D', 'migration', 'maneuvers', 'such', 'as', 'precise', 'angular', 'turns', 'and', 'migration', 'in', 'a', 'complete', 'circle', '.']\n",
      "['Our', 'stimulation', 'scheme', 'proves', 'that', 'cells', 'effectively', 'time-average', 'electric', 'field', 'cues', ',', 'helping', 'to', 'elucidate', 'the', 'transduction', 'timescales', 'in', 'electrotaxis', '.']\n",
      "['Together', ',', 'this', 'work', 'represents', 'an', 'enabling', 'platform', 'for', 'controlling', 'cell', 'migration', 'with', 'broad', 'utility', 'across', 'many', 'cell', 'types', '.']\n"
     ]
    }
   ],
   "source": [
    "for txt in tokenized_text:\n",
    "    print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentence = []\n",
    "input_ids = []\n",
    "for sent in text_split:\n",
    "    tokenized_sentence.extend(tokenizer.tokenize(sent))\n",
    "    \n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(tokenized_sentence) ],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
    "                          truncating=\"post\", padding=\"post\")\n",
    "attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]\n",
    "\n",
    "input_ids = torch.tensor(input_ids).cuda()\n",
    "attention_masks = torch.tensor(attention_masks).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig.from_json_file('biobert_v1.0_pubmed_pmc/bert_config.json')\n",
    "vocab_len = 65\n",
    "model = ner_bio(vocab_len,config,state_dict=None)\n",
    "model.load_state_dict(torch.load('app/BIONER_classifier.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "text = \"\"\"Directed cell migration is critical across biological processes spanning healing to cancer invasion, yet no existing tools allow real-time interactive guidance over such migration. We present a new bioreactor that harnesses electrotaxis—directed cell migration along electric field gradients—by integrating four independent electrodes under computer control to dynamically program electric field patterns, and hence steer cell migration. Using this platform, we programmed and characterized multiple precise, two-dimensional collective migration maneuvers in renal epithelia and primary skin keratinocyte ensembles. First, we demonstrated on-demand, 90-degree collective turning. Next, we developed a universal electrical stimulation scheme capable of programming arbitrary 2D migration maneuvers such as precise angular turns and migration in a complete circle. Our stimulation scheme proves that cells effectively time-average electric field cues, helping to elucidate the transduction timescales in electrotaxis. Together, this work represents an enabling platform for controlling cell migration with broad utility across many cell types.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_text = nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    _, y_hat = model(input_ids,attention_mask=attention_masks)\n",
    "label_indices = y_hat.to('cpu').numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
    "new_tokens, new_labels = [], []\n",
    "for token, label_idx in zip(tokens, label_indices[0]):\n",
    "    if token.startswith(\"##\"):\n",
    "        new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "    else:\n",
    "        new_labels.append(tag_values[label_idx])\n",
    "        new_tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-Immaterial_anatomical_entity\tRegulation\n",
      "B-Immaterial_anatomical_entity\tof\n",
      "B-Immaterial_anatomical_entity\tSaccharomyces\n",
      "B-Immaterial_anatomical_entity\tcerevisiae\n",
      "E-Amino_acid\tkinetochores\n",
      "B-Immaterial_anatomical_entity\tby\n",
      "B-Immaterial_anatomical_entity\tthe\n",
      "B-Immaterial_anatomical_entity\ttype\n",
      "B-Immaterial_anatomical_entity\t1\n",
      "B-Immaterial_anatomical_entity\tphosphatase\n",
      "B-Immaterial_anatomical_entity\tGlc7p\n",
      "B-Immaterial_anatomical_entity\t.\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n",
      "B-Anatomical_system\t[PAD]\n"
     ]
    }
   ],
   "source": [
    "for token, label in zip(new_tokens, new_labels):\n",
    "    print(\"{}\\t{}\".format(label, token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
